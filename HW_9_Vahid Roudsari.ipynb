{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92703904",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Question 1: What is the difference between **Covariance** and **Correlation**?\n",
    "\n",
    "### Covariance\n",
    "\n",
    "- Measures how **two variables change together**\n",
    "- **Scale-dependent**\n",
    "- Indicates direction, not strength\n",
    "\n",
    "$$\n",
    "\\mathrm{Cov}(X,Y) = \\mathbb{E}\\left[(X-\\mu_X)(Y-\\mu_Y)\\right]\n",
    "$$\n",
    "\n",
    "### Correlation\n",
    "\n",
    "- **Normalized covariance**\n",
    "- Always in the range **[−1, +1]**\n",
    "- **Scale-independent**\n",
    "- Measures **strength and direction of linear relationship**\n",
    "\n",
    "$$\n",
    "\\rho_{X,Y} = \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\n",
    "$$\n",
    "\n",
    "**Key takeaway:**\n",
    "\n",
    "> Covariance shows direction; correlation shows direction **and strength**.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 2: What is the difference between **Dependency** and **Correlation**?\n",
    "\n",
    "### Dependency\n",
    "\n",
    "- A **general concept**\n",
    "- Includes **linear and non-linear** relationships\n",
    "\n",
    "### Correlation\n",
    "\n",
    "- Measures **only linear dependency**\n",
    "\n",
    "**Important insight:**\n",
    "\n",
    "Two variables can be **dependent but uncorrelated**, for example:\n",
    "\n",
    "$$\n",
    "Y = X^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Question 3: What is the difference between **Regression** and **Classification**?\n",
    "\n",
    "| Aspect | Regression | Classification |\n",
    "|------|-----------|----------------|\n",
    "| Output | Continuous | Discrete classes |\n",
    "| Example | Price, temperature | Spam / Not spam |\n",
    "| Loss | MSE, MAE | Cross-Entropy |\n",
    "| Goal | Predict a value | Assign a class |\n",
    "\n",
    "---\n",
    "\n",
    "## Question 4: What is the effect of **noise** on **bias–variance**?\n",
    "\n",
    "- Noise **does not affect bias**\n",
    "- Noise increases **irreducible variance**\n",
    "\n",
    "$$\n",
    "\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Noise}\n",
    "$$\n",
    "\n",
    "**Key insight:**\n",
    "\n",
    "> No model can eliminate intrinsic data noise.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 5: What is **uncertainty**?\n",
    "\n",
    "What affects **model uncertainty** and **data uncertainty**?\n",
    "\n",
    "### Model Uncertainty (Epistemic)\n",
    "\n",
    "Caused by:\n",
    "- Limited data\n",
    "- Poor model choice\n",
    "- Unstable parameters\n",
    "\n",
    "✔ Can be reduced with more data or better models\n",
    "\n",
    "### Data Uncertainty (Aleatoric)\n",
    "\n",
    "- Inherent randomness\n",
    "- Measurement noise\n",
    "\n",
    "✘ Cannot be eliminated\n",
    "\n",
    "---\n",
    "\n",
    "## Question 6: What is the difference between **uncertainty** and **high dimensionality**?\n",
    "\n",
    "### High Dimensionality\n",
    "\n",
    "- Large number of features\n",
    "- Leads to the **curse of dimensionality**\n",
    "\n",
    "### Uncertainty\n",
    "\n",
    "- Lack of confidence in predictions\n",
    "\n",
    "### Relationship\n",
    "\n",
    "- High dimension + small dataset ⇒ high uncertainty  \n",
    "- High dimension + enough data ⇒ uncertainty may be low\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "> High dimensionality does not always imply high uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 7: What is the difference between **white noise** and **colored noise**?\n",
    "\n",
    "### White Noise\n",
    "\n",
    "- Equal power at all frequencies\n",
    "- No temporal correlation\n",
    "\n",
    "### Colored Noise\n",
    "\n",
    "- Power varies with frequency\n",
    "- Has structure and correlation\n",
    "\n",
    "| Type | Property |\n",
    "|----|---------|\n",
    "| White | Pure randomness |\n",
    "| Pink | More low-frequency power |\n",
    "| Brown | Strong temporal correlation |\n",
    "\n",
    "---\n",
    "\n",
    "## Question 8: What is the difference between **noise** and an **outlier**?\n",
    "\n",
    "### Noise\n",
    "\n",
    "- Small, random fluctuations\n",
    "- Common and spread across data\n",
    "\n",
    "### Outlier\n",
    "\n",
    "- Rare, extreme values\n",
    "- May be errors or meaningful events\n",
    "\n",
    "---\n",
    "\n",
    "## Question 9: What is the difference between **noise** and an **anomaly**?\n",
    "\n",
    "### Noise\n",
    "\n",
    "- Random and meaningless\n",
    "- Small impact\n",
    "\n",
    "### Anomaly\n",
    "\n",
    "- Rare but **meaningful**\n",
    "- Indicates abnormal system behavior\n",
    "\n",
    "---\n",
    "\n",
    "## Question 10: What is the difference between an **outlier** and an **anomaly**?\n",
    "\n",
    "| Aspect | Outlier | Anomaly |\n",
    "|-----|--------|---------|\n",
    "| Nature | Statistical deviation | Contextual abnormal behavior |\n",
    "| Meaning | May be error | Usually important |\n",
    "| Handling | Often removed | Usually investigated |\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- Outlier: Incorrect sensor reading  \n",
    "- Anomaly: Fraudulent transaction  \n",
    "\n",
    "---\n",
    "\n",
    "## Final Professional Summary\n",
    "\n",
    "- **Noise** → random error  \n",
    "- **Outlier** → extreme data point  \n",
    "- **Anomaly** → meaningful abnormal pattern  \n",
    "- **Correlation** → linear dependency  \n",
    "- **Dependency** → any relationship  \n",
    "- **Uncertainty** → lack of confidence (model or data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb087ebf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0420ca24",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f349b698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# تعداد نمونه\n",
    "N = 1000\n",
    "\n",
    "# متغیر پایه\n",
    "x = np.random.uniform(0, 2*np.pi, N)\n",
    "\n",
    "# فیچرها\n",
    "x1 = x**3\n",
    "x2 = np.sin(x)\n",
    "\n",
    "# نویز\n",
    "mu = 1\n",
    "sigma2 = 0.2\n",
    "noise = np.random.normal(mu, np.sqrt(sigma2), N)\n",
    "\n",
    "# خروجی واقعی\n",
    "y = 2 - x1 + 3*x2 + noise\n",
    "\n",
    "# تقسیم Train / Test\n",
    "X1 = x2.reshape(-1, 1)        # فقط x2\n",
    "X2 = np.column_stack([x1, x2])  # x1 و x2\n",
    "\n",
    "X1_train, X1_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=42)\n",
    "X2_train, X2_test, _, _ = train_test_split(X2, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a0b540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 6541.2952397769295 80.87827421364113 1.1143142542347606\n",
      "Test : 5486.323916490389 74.0697233455775 1.0909649774903394\n"
     ]
    }
   ],
   "source": [
    "# مدل بدون بایاس (چون b=2 ثابت است)\n",
    "model_a = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# حذف بایاس از y\n",
    "y_train_adj = y_train - 2\n",
    "y_test_adj  = y_test  - 2\n",
    "\n",
    "model_a.fit(X1_train, y_train_adj)\n",
    "\n",
    "y_train_pred = model_a.predict(X1_train) + 2\n",
    "y_test_pred  = model_a.predict(X1_test)  + 2\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    ndei = rmse / np.std(y_true)\n",
    "    return mse, rmse, ndei\n",
    "\n",
    "mse_tr, rmse_tr, ndei_tr = metrics(y_train, y_train_pred)\n",
    "mse_te, rmse_te, ndei_te = metrics(y_test, y_test_pred)\n",
    "\n",
    "print(\"Train:\", mse_tr, rmse_tr, ndei_tr)\n",
    "print(\"Test :\", mse_te, rmse_te, ndei_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "546ebdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (scaled): (6420.91415312053, np.float64(80.13060684358088), np.float64(1.1040131391827306))\n",
      "Test  (scaled): (5384.32118414231, np.float64(73.37793390483483), np.float64(1.080775685326702))\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X1_train_s = scaler.fit_transform(X1_train)\n",
    "X1_test_s  = scaler.transform(X1_test)\n",
    "\n",
    "model_a_s = LinearRegression(fit_intercept=False)\n",
    "model_a_s.fit(X1_train_s, y_train_adj)\n",
    "\n",
    "y_train_pred_s = model_a_s.predict(X1_train_s) + 2\n",
    "y_test_pred_s  = model_a_s.predict(X1_test_s)  + 2\n",
    "\n",
    "print(\"Train (scaled):\", metrics(y_train, y_train_pred_s))\n",
    "print(\"Test  (scaled):\", metrics(y_test, y_test_pred_s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a3621c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4.061958970807997, np.float64(2.015430219781374), np.float64(0.027767934518304734))\n",
      "Test : (4.2235362815530175, np.float64(2.055124395639597), np.float64(0.030269705876559992))\n"
     ]
    }
   ],
   "source": [
    "#هر دو فیچر، بدون بایاس\n",
    "model_b = LinearRegression(fit_intercept=False)\n",
    "model_b.fit(X2_train, y_train)\n",
    "\n",
    "y_train_pred = model_b.predict(X2_train)\n",
    "y_test_pred  = model_b.predict(X2_test)\n",
    "\n",
    "print(\"Train:\", metrics(y_train, y_train_pred))\n",
    "print(\"Test :\", metrics(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedf7660",
   "metadata": {},
   "source": [
    "## هر دو فیچر + بایاس آزاد"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1dcc702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (0.20047462544003955, np.float64(0.44774392842342325), np.float64(0.006168868543996932))\n",
      "Test : (0.1712434563169082, np.float64(0.41381572748858664), np.float64(0.006095047280228429))\n",
      "Estimated bias: 3.0775816678770767\n",
      "Estimated weights: [-1.00050827  2.98714397]\n"
     ]
    }
   ],
   "source": [
    "model_c = LinearRegression(fit_intercept=True)\n",
    "model_c.fit(X2_train, y_train)\n",
    "\n",
    "y_train_pred = model_c.predict(X2_train)\n",
    "y_test_pred  = model_c.predict(X2_test)\n",
    "\n",
    "print(\"Train:\", metrics(y_train, y_train_pred))\n",
    "print(\"Test :\", metrics(y_test, y_test_pred))\n",
    "\n",
    "print(\"Estimated bias:\", model_c.intercept_)\n",
    "print(\"Estimated weights:\", model_c.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ca8730",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "053f66e7",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "| تغییر نویز | اثر روی ضرایب         | اثر روی MSE      |\n",
    "| ---------- | --------------------- | ---------------- |\n",
    "| ↑ μ        | جذب توسط b            | MSE تقریباً ثابت |\n",
    "| ↑ σ²       | ضرایب پایدار          | MSE ↑            |\n",
    "| ↓ σ²       | نزدیک‌تر به مدل واقعی | MSE ↓            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f993594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma^2=0.05 → MSE=0.052, RMSE=0.227, NDEI=0.003\n",
      "sigma^2=0.20 → MSE=0.227, RMSE=0.476, NDEI=0.007\n",
      "sigma^2=1.00 → MSE=0.914, RMSE=0.956, NDEI=0.014\n"
     ]
    }
   ],
   "source": [
    "sigma2_list = [0.05, 0.2, 1.0]\n",
    "\n",
    "for s2 in sigma2_list:\n",
    "    noise = np.random.normal(mu, np.sqrt(s2), N)\n",
    "    y_noisy = 2 - x1 + 3*x2 + noise\n",
    "\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X2, y_noisy, test_size=0.2)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(Xtr, ytr)\n",
    "\n",
    "    y_pred = model.predict(Xte)\n",
    "    mse, rmse, ndei = metrics(yte, y_pred)\n",
    "\n",
    "    print(f\"sigma^2={s2:.2f} → MSE={mse:.3f}, RMSE={rmse:.3f}, NDEI={ndei:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea8a9b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "θ_LS = [ 3.07815123 -1.00055281  2.97192212]\n"
     ]
    }
   ],
   "source": [
    "# حل تحلیلی Least Squares (ماتریسی)\n",
    "# افزودن ستون 1 برای بایاس\n",
    "X_ls = np.column_stack([np.ones(N), x1, x2])\n",
    "\n",
    "theta_ls = np.linalg.inv(X_ls.T @ X_ls) @ X_ls.T @ y\n",
    "\n",
    "print(\"θ_LS =\", theta_ls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc6ff80",
   "metadata": {},
   "source": [
    "| مدل    | وضعیت بایاس      | کیفیت               |\n",
    "| ------ | ---------------- | ------------------- |\n",
    "| (الف)  | ثابت             | ضعیف (Underfit)     |\n",
    "| (ب)    | حذف شده          | Bias Error بالا     |\n",
    "| (ج)    | تخمین زده می‌شود | **بهترین مدل**      |\n",
    "| نویز ↑ | –                | MSE ↑               |\n",
    "| نویز ↓ | –                | Generalization بهتر |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b546d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84170a66",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92068dd9",
   "metadata": {},
   "source": [
    "<div dir=rtl >\n",
    "\n",
    "## 2 تمرین شماره :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef700a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape raw: (101766, 50)\n",
      "Train shape: (81412, 50) Test shape: (20354, 50)\n",
      "Positive rate (train): 0.11160516877118852 Positive rate (test): 0.11157512036946055\n",
      "\n",
      "======================================================================\n",
      "Feature Set: ALL | #cols=50\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    RocCurveDisplay,\n",
    "    PrecisionRecallDisplay\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    " \n",
    "\n",
    "\n",
    "# 1) تنظیمات کلی پروژه\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "DATA_PATH = \"diabetic_data.csv\"  # مسیر فایل csv\n",
    "ARTIFACT_DIR = \"artifacts_diabetes_readmission\"\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# 2) توابع کمکی: پاکسازی، ساخت هدف، و Encode ICD-9\n",
    "\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    داده را از CSV می‌خواند.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"فایل دیتاست پیدا نشد: {path}\\n\"\n",
    "            \"لطفاً diabetic_data.csv را کنار اسکریپت قرار دهید یا مسیر صحیح بدهید.\"\n",
    "        )\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_question_marks_with_nan(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    در این دیتاست مقدار '?' به معنی داده گم‌شده است.\n",
    "    آن را به NaN تبدیل می‌کنیم تا مراحل Imputation درست انجام شود.\n",
    "    \"\"\"\n",
    "    df = df.replace(\"?\", np.nan)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_target_readmitted_30(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ستون readmitted معمولاً یکی از مقادیر:\n",
    "    '<30', '>30', 'NO' است.\n",
    "\n",
    "    هدف: 1 اگر <30 باشد، در غیر این صورت 0\n",
    "    \"\"\"\n",
    "    if \"readmitted\" not in df.columns:\n",
    "        raise ValueError(\"ستون readmitted در دیتاست وجود ندارد.\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"target_readmit_30\"] = (df[\"readmitted\"] == \"<30\").astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def icd9_to_group(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Encode کردن ICD-9 به گروه‌های قابل استفاده برای مدل.\n",
    "\n",
    "    ایده: کدهای ICD-9 را به دسته‌های سطح بالا تبدیل می‌کنیم تا:\n",
    "    1) مدل با یک متغیر متنیِ پرنویز مواجه نشود\n",
    "    2) اطلاعات بالینی همچنان حفظ شود\n",
    "    3) تعداد دسته‌ها محدود و پایدار باشد\n",
    "\n",
    "    قواعد متداول:\n",
    "    - کدهای عددی را به بازه‌های ICD-9 تبدیل می‌کنیم\n",
    "    - کدهای V و E گروه جداگانه می‌گیرند\n",
    "    - مقادیر NaN یا نامعتبر -> 'UNKNOWN'\n",
    "    \"\"\"\n",
    "    if pd.isna(code):\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "    code = str(code).strip()\n",
    "\n",
    "    # دسته‌های خاص\n",
    "    if code.startswith(\"V\"):\n",
    "        return \"V_CODE\"\n",
    "    if code.startswith(\"E\"):\n",
    "        return \"E_CODE\"\n",
    "\n",
    "    # تلاش برای تبدیل به عدد (بعضی کدها مثل '250.13' هستند)\n",
    "    try:\n",
    "        num = float(code)\n",
    "    except ValueError:\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "    # بازه‌بندی ICD-9 (سطح بالا)\n",
    "    # 001–139: Infectious and parasitic diseases\n",
    "    if 1 <= num <= 139:\n",
    "        return \"INFECTIOUS\"\n",
    "    # 140–239: Neoplasms\n",
    "    if 140 <= num <= 239:\n",
    "        return \"NEOPLASMS\"\n",
    "    # 240–279: Endocrine, nutritional and metabolic diseases (دیابت هم در 250 است)\n",
    "    if 240 <= num <= 279:\n",
    "        return \"ENDO_METABOLIC\"\n",
    "    # 280–289: Diseases of the blood\n",
    "    if 280 <= num <= 289:\n",
    "        return \"BLOOD\"\n",
    "    # 290–319: Mental disorders\n",
    "    if 290 <= num <= 319:\n",
    "        return \"MENTAL\"\n",
    "    # 320–389: Nervous system and sense organs\n",
    "    if 320 <= num <= 389:\n",
    "        return \"NERVOUS\"\n",
    "    # 390–459: Circulatory system\n",
    "    if 390 <= num <= 459:\n",
    "        return \"CIRCULATORY\"\n",
    "    # 460–519: Respiratory system\n",
    "    if 460 <= num <= 519:\n",
    "        return \"RESPIRATORY\"\n",
    "    # 520–579: Digestive system\n",
    "    if 520 <= num <= 579:\n",
    "        return \"DIGESTIVE\"\n",
    "    # 580–629: Genitourinary system\n",
    "    if 580 <= num <= 629:\n",
    "        return \"GENITOURINARY\"\n",
    "    # 630–679: Pregnancy/childbirth\n",
    "    if 630 <= num <= 679:\n",
    "        return \"PREGNANCY\"\n",
    "    # 680–709: Skin and subcutaneous tissue\n",
    "    if 680 <= num <= 709:\n",
    "        return \"SKIN\"\n",
    "    # 710–739: Musculoskeletal\n",
    "    if 710 <= num <= 739:\n",
    "        return \"MUSCULOSKELETAL\"\n",
    "    # 740–759: Congenital anomalies\n",
    "    if 740 <= num <= 759:\n",
    "        return \"CONGENITAL\"\n",
    "    # 760–779: Perinatal conditions\n",
    "    if 760 <= num <= 779:\n",
    "        return \"PERINATAL\"\n",
    "    # 780–799: Symptoms, signs, ill-defined conditions\n",
    "    if 780 <= num <= 799:\n",
    "        return \"SYMPTOMS\"\n",
    "    # 800–999: Injury and poisoning\n",
    "    if 800 <= num <= 999:\n",
    "        return \"INJURY\"\n",
    "\n",
    "    return \"OTHER\"\n",
    "\n",
    "\n",
    "def encode_icd_columns(df: pd.DataFrame, cols=(\"diag_1\", \"diag_2\", \"diag_3\")) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ستون‌های تشخیصی diag_1, diag_2, diag_3 را به گروه ICD تبدیل می‌کنیم.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c + \"_group\"] = df[c].apply(icd9_to_group)\n",
    "        else:\n",
    "            # اگر ستون وجود نداشت، رد می‌کنیم\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_leaky_or_id_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ستون‌هایی که معمولاً شناسه‌ای هستند یا ارزش پیش‌بینی ندارند را حذف می‌کنیم.\n",
    "    (این انتخاب می‌تواند پروژه‌ای باشد؛ اینجا یک انتخاب رایج و امن انجام می‌دهیم)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # شناسه‌ها و ستون‌های غالباً غیرمفید/خاص دیتاست\n",
    "    drop_cols = [\n",
    "        \"encounter_id\",\n",
    "        \"patient_nbr\",\n",
    "        \"readmitted\"  # چون از آن هدف ساختیم\n",
    "    ]\n",
    "    existing = [c for c in drop_cols if c in df.columns]\n",
    "    df = df.drop(columns=existing, errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_feature_sets(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    X و y را جدا می‌کنیم.\n",
    "    \"\"\"\n",
    "    if \"target_readmit_30\" not in df.columns:\n",
    "        raise ValueError(\"target_readmit_30 ساخته نشده است.\")\n",
    "\n",
    "    y = df[\"target_readmit_30\"].astype(int)\n",
    "    X = df.drop(columns=[\"target_readmit_30\"])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def get_main_feature_subset_columns(X: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    لیست فیچرهای «کلیدی» پیشنهادی:\n",
    "    - جنسیت، سن، نژاد (اگر موجود باشد)\n",
    "    - گروه‌های ICD (diag_1_group, diag_2_group, diag_3_group)\n",
    "    - تعداد بستری‌ها/ویزیت‌ها/آزمایش‌ها/داروها (اگر ستون‌هایشان باشد)\n",
    "    - زمان بستری (time_in_hospital) و ... (در صورت وجود)\n",
    "\n",
    "    هدف: ساخت مدل سبک‌تر و قابل توضیح‌تر.\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        \"gender\",\n",
    "        \"age\",\n",
    "        \"race\",\n",
    "        \"time_in_hospital\",\n",
    "        \"num_lab_procedures\",\n",
    "        \"num_procedures\",\n",
    "        \"num_medications\",\n",
    "        \"number_outpatient\",\n",
    "        \"number_emergency\",\n",
    "        \"number_inpatient\",\n",
    "        \"diag_1_group\",\n",
    "        \"diag_2_group\",\n",
    "        \"diag_3_group\",\n",
    "        \"admission_type_id\",\n",
    "        \"discharge_disposition_id\",\n",
    "        \"admission_source_id\",\n",
    "        \"change\",\n",
    "        \"diabetesMed\"\n",
    "    ]\n",
    "    return [c for c in candidates if c in X.columns]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) ساخت Preprocessor (عددی/دسته‌ای) به صورت استاندارد\n",
    "# ------------------------------------------------------------\n",
    "def build_preprocessor(X: pd.DataFrame) -> ColumnTransformer:\n",
    "    \"\"\"\n",
    "    برای ستون‌های عددی: impute با median\n",
    "    برای ستون‌های دسته‌ای: impute با most_frequent و سپس OneHot\n",
    "    \"\"\"\n",
    "    # تشخیص ستون‌ها\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = [c for c in X.columns if c not in numeric_cols]\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_cols),\n",
    "            (\"cat\", categorical_transformer, categorical_cols)\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) متریک‌ها و ارزیابی (تمرکز بر Sensitivity/Recall)\n",
    "# ------------------------------------------------------------\n",
    "def evaluate_classifier(name: str, model, X_test: pd.DataFrame, y_test: pd.Series) -> dict:\n",
    "    \"\"\"\n",
    "    ارزیابی استاندارد برای مسئله نامتوازن:\n",
    "    - ROC-AUC\n",
    "    - PR-AUC (Average Precision)\n",
    "    - گزارش طبقه‌بندی و ماتریس درهم‌ریختگی\n",
    "    - تمرکز ویژه روی Recall کلاس 1 (Sensitivity)\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # برای AUC باید احتمال داشته باشیم\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        # برخی مدل‌ها ممکن است decision_function داشته باشند\n",
    "        if hasattr(model, \"decision_function\"):\n",
    "            scores = model.decision_function(X_test)\n",
    "            # تبدیل تقریبی به [0,1] برای PR/AUC (اگر لازم باشد)\n",
    "            y_proba = (scores - scores.min()) / (scores.max() - scores.min() + 1e-9)\n",
    "        else:\n",
    "            y_proba = None\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    # Sensitivity = Recall کلاس 1\n",
    "    sensitivity = report.get(\"1\", {}).get(\"recall\", np.nan)\n",
    "\n",
    "    out = {\n",
    "        \"model_name\": name,\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"classification_report\": report,\n",
    "        \"sensitivity_recall_class_1\": float(sensitivity)\n",
    "    }\n",
    "\n",
    "    if y_proba is not None:\n",
    "        out[\"roc_auc\"] = float(roc_auc_score(y_test, y_proba))\n",
    "        out[\"pr_auc\"] = float(average_precision_score(y_test, y_proba))\n",
    "    else:\n",
    "        out[\"roc_auc\"] = None\n",
    "        out[\"pr_auc\"] = None\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_curves(model, X_test, y_test, title_prefix: str, out_dir: str):\n",
    "    \"\"\"\n",
    "    رسم ROC و Precision-Recall (برای دیتای نامتوازن بسیار مهم است)\n",
    "    \"\"\"\n",
    "    if not hasattr(model, \"predict_proba\"):\n",
    "        return\n",
    "\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # ROC\n",
    "    plt.figure()\n",
    "    RocCurveDisplay.from_predictions(y_test, y_proba)\n",
    "    plt.title(f\"{title_prefix} - ROC Curve\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, f\"{title_prefix}_roc.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # PR\n",
    "    plt.figure()\n",
    "    PrecisionRecallDisplay.from_predictions(y_test, y_proba)\n",
    "    plt.title(f\"{title_prefix} - Precision-Recall Curve\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, f\"{title_prefix}_pr.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# 5) آموزش RandomForest با max_leaf_nodes = 100 و 300\n",
    "\n",
    "def train_random_forest_models(X_train, y_train, X_test, y_test, feature_set_name: str, preprocessor):\n",
    "    \"\"\"\n",
    "    دو مدل RF با تعداد برگ‌های متفاوت می‌سازیم و ارزیابی می‌کنیم.\n",
    "    همچنین برای مقابله با Imbalance از class_weight='balanced' استفاده می‌کنیم.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    models = {}\n",
    "\n",
    "    for max_leaf_nodes in [100, 300]:\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=400,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "            max_leaf_nodes=max_leaf_nodes,   # حساسیت به تعداد برگ‌ها\n",
    "            class_weight=\"balanced\"          # کمک برای دیتای نامتوازن\n",
    "        )\n",
    "\n",
    "        pipe = Pipeline(steps=[\n",
    "            (\"preprocess\", preprocessor),\n",
    "            (\"model\", clf)\n",
    "        ])\n",
    "\n",
    "        name = f\"RF_leaf{max_leaf_nodes}_{feature_set_name}\"\n",
    "        pipe.fit(X_train, y_train)\n",
    "\n",
    "        metrics = evaluate_classifier(name, pipe, X_test, y_test)\n",
    "        results.append(metrics)\n",
    "        models[name] = pipe\n",
    "\n",
    "        # ذخیره مدل\n",
    "        joblib.dump(pipe, os.path.join(ARTIFACT_DIR, f\"{name}.joblib\"))\n",
    "\n",
    "        # ذخیره نمودارها\n",
    "        plot_curves(pipe, X_test, y_test, title_prefix=name, out_dir=ARTIFACT_DIR)\n",
    "\n",
    "    return results, models\n",
    "\n",
    "\n",
    "\n",
    "# 6) آموزش XGBoost با max_leaves = 100 و 300 (grow_policy=lossguide)\n",
    "\n",
    "def train_xgboost_models(X_train, y_train, X_test, y_test, feature_set_name: str, preprocessor):\n",
    "    \"\"\"\n",
    "    اگر XGBoost نصب باشد:\n",
    "    - از grow_policy='lossguide' استفاده می‌کنیم تا max_leaves معنی‌دار شود.\n",
    "    - برای Imbalance از scale_pos_weight استفاده می‌کنیم.\n",
    "    \"\"\"\n",
    "    \n",
    "    # نسبت منفی به مثبت برای scale_pos_weight\n",
    "    pos = (y_train == 1).sum()\n",
    "    neg = (y_train == 0).sum()\n",
    "    scale_pos_weight = float(neg / (pos + 1e-9))\n",
    "\n",
    "    results = []\n",
    "    models = {}\n",
    "\n",
    "    for max_leaves in [100, 300]:\n",
    "        xgb = XGBClassifier(\n",
    "            n_estimators=800,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            random_state=RANDOM_STATE,\n",
    "\n",
    "            # کنترل ساختار درخت\n",
    "            grow_policy=\"lossguide\",\n",
    "            max_leaves=max_leaves,\n",
    "\n",
    "            # برای جلوگیری از overfitting\n",
    "            reg_lambda=1.0,\n",
    "            reg_alpha=0.0,\n",
    "\n",
    "            # برای دیتای نامتوازن\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "\n",
    "            # سرعت/پایداری\n",
    "            tree_method=\"hist\",\n",
    "            eval_metric=\"logloss\"\n",
    "        )\n",
    "\n",
    "        pipe = Pipeline(steps=[\n",
    "            (\"preprocess\", preprocessor),\n",
    "            (\"model\", xgb)\n",
    "        ])\n",
    "\n",
    "        name = f\"XGB_leaves{max_leaves}_{feature_set_name}\"\n",
    "        pipe.fit(X_train, y_train)\n",
    "\n",
    "        metrics = evaluate_classifier(name, pipe, X_test, y_test)\n",
    "        results.append(metrics)\n",
    "        models[name] = pipe\n",
    "\n",
    "        # ذخیره کل پایپلاین (پیش‌پردازش + مدل)\n",
    "        joblib.dump(pipe, os.path.join(ARTIFACT_DIR, f\"{name}.joblib\"))\n",
    "\n",
    "        # ذخیره مدل XGB به فرمت json هم (قابل حمل‌تر برای سرویس‌دهی)\n",
    "        xgb_model = pipe.named_steps[\"model\"]\n",
    "        xgb_model.save_model(os.path.join(ARTIFACT_DIR, f\"{name}_xgb.json\"))\n",
    "\n",
    "        # نمودارها\n",
    "        plot_curves(pipe, X_test, y_test, title_prefix=name, out_dir=ARTIFACT_DIR)\n",
    "\n",
    "    return results, models\n",
    "\n",
    "\n",
    "\n",
    "# 7) انتخاب فیچرهای مهم (Feature Selection) با Permutation Importance\n",
    "\n",
    "def select_top_features_with_permutation_importance(\n",
    "    trained_pipeline: Pipeline,\n",
    "    X_valid: pd.DataFrame,\n",
    "    y_valid: pd.Series,\n",
    "    top_k: int = 30\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    برای اینکه Feature Selection «سازگار با پیش‌پردازش OneHot» باشد:\n",
    "    - از permutation_importance روی خود Pipeline استفاده می‌کنیم\n",
    "    - اما خروجی permutation_importance به سطح ستون‌های بعد از OneHot مربوط است\n",
    "\n",
    "    راه حل عملی:\n",
    "    - اگر هدف شما \"ستون‌های اصلی\" باشد، بهتر است از قبل subset تعریف کنید (مثل gender, age, diag_group)\n",
    "    - اگر top features واقعی بعد از OneHot بخواهید، باید نام فیچرهای OneHot را استخراج کنید و نگه دارید\n",
    "      (که اینجا به صورت گزارش و Explainability خوب است)\n",
    "\n",
    "    در این پروژه، برای Feature Set دوم (Most Important) همان subset بالینی-توضیح‌پذیر را استفاده می‌کنیم.\n",
    "    با این حال، این تابع را برای گزارش اهمیت‌ها نگه می‌داریم.\n",
    "    \"\"\"\n",
    "    r = permutation_importance(\n",
    "        trained_pipeline,\n",
    "        X_valid,\n",
    "        y_valid,\n",
    "        n_repeats=5,\n",
    "        random_state=RANDOM_STATE,\n",
    "        scoring=\"average_precision\"  # مناسب دیتای نامتوازن\n",
    "    )\n",
    "\n",
    "    # گرفتن نام فیچرها بعد از preprocess\n",
    "    pre = trained_pipeline.named_steps[\"preprocess\"]\n",
    "\n",
    "    # استخراج feature names\n",
    "    feature_names = []\n",
    "    try:\n",
    "        # num names\n",
    "        num_cols = pre.transformers_[0][2]\n",
    "        feature_names.extend(list(num_cols))\n",
    "\n",
    "        # cat names (بعد از onehot)\n",
    "        cat_pipe = pre.transformers_[1][1]\n",
    "        ohe = cat_pipe.named_steps[\"onehot\"]\n",
    "        cat_cols = pre.transformers_[1][2]\n",
    "        ohe_names = ohe.get_feature_names_out(cat_cols).tolist()\n",
    "        feature_names.extend(ohe_names)\n",
    "    except Exception:\n",
    "        # اگر به هر دلیل نتوانستیم نام‌ها را بگیریم\n",
    "        feature_names = [f\"f_{i}\" for i in range(len(r.importances_mean))]\n",
    "\n",
    "    importances = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance_mean\": r.importances_mean\n",
    "    }).sort_values(by=\"importance_mean\", ascending=False)\n",
    "\n",
    "    # ذخیره گزارش\n",
    "    importances.to_csv(os.path.join(ARTIFACT_DIR, \"permutation_importances.csv\"), index=False)\n",
    "\n",
    "    return importances.head(top_k)[\"feature\"].tolist()\n",
    "\n",
    "\n",
    "\n",
    "# 8) بررسی Bias (ساده و عملی): متریک به تفکیک گروه‌ها\n",
    "\n",
    "def subgroup_performance(model: Pipeline, X_test: pd.DataFrame, y_test: pd.Series, group_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    بررسی ساده‌ی بایاس: عملکرد مدل در گروه‌های مختلف (مثلاً gender یا race یا age)\n",
    "    خروجی: recall/precision/F1 برای کلاس 1 در هر گروه\n",
    "    \"\"\"\n",
    "    if group_col not in X_test.columns:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rows = []\n",
    "    for g, idx in X_test.groupby(group_col).groups.items():\n",
    "        Xi = X_test.loc[idx]\n",
    "        yi = y_test.loc[idx]\n",
    "        yp = model.predict(Xi)\n",
    "        rep = classification_report(yi, yp, output_dict=True, zero_division=0)\n",
    "        rows.append({\n",
    "            \"group_col\": group_col,\n",
    "            \"group_value\": str(g),\n",
    "            \"n\": int(len(yi)),\n",
    "            \"recall_class1\": rep.get(\"1\", {}).get(\"recall\", np.nan),\n",
    "            \"precision_class1\": rep.get(\"1\", {}).get(\"precision\", np.nan),\n",
    "            \"f1_class1\": rep.get(\"1\", {}).get(\"f1-score\", np.nan),\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values(by=\"n\", ascending=False)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 9) اجرای اصلی پروژه\n",
    "# ------------------------------------------------------------\n",
    "def main():\n",
    "    # -------------------------\n",
    "    # 9-1) Load\n",
    "    # -------------------------\n",
    "    df = load_data(DATA_PATH)\n",
    "    print(\"Shape raw:\", df.shape)\n",
    "\n",
    "    # -------------------------\n",
    "    # 9-2) Clean: '?' -> NaN\n",
    "    # -------------------------\n",
    "    df = replace_question_marks_with_nan(df)\n",
    "\n",
    "    # -------------------------\n",
    "    # 9-3) Encode ICD ستون‌ها\n",
    "    # -------------------------\n",
    "    df = encode_icd_columns(df, cols=(\"diag_1\", \"diag_2\", \"diag_3\"))\n",
    "\n",
    "    # -------------------------\n",
    "    # 9-4) ساخت هدف readmission <30\n",
    "    # -------------------------\n",
    "    df = build_target_readmitted_30(df)\n",
    "\n",
    "    # -------------------------\n",
    "    # 9-5) حذف ستون‌های ID و leakage\n",
    "    # -------------------------\n",
    "    df = drop_leaky_or_id_columns(df)\n",
    "\n",
    "    # -------------------------\n",
    "    # 9-6) جدا کردن X و y\n",
    "    # -------------------------\n",
    "    X, y = make_feature_sets(df)\n",
    "\n",
    "    # -------------------------\n",
    "    # 9-7) Split\n",
    "    # -------------------------\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y  # چون داده نامتوازن است، stratify مهم است\n",
    "    )\n",
    "\n",
    "    print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "    print(\"Positive rate (train):\", y_train.mean(), \"Positive rate (test):\", y_test.mean())\n",
    "\n",
    "    # -------------------------\n",
    "    # 9-8) Feature sets\n",
    "    #   الف) همه فیچرها\n",
    "    #   ب) فیچرهای مهم/بالینی\n",
    "    # -------------------------\n",
    "    feature_sets = {\n",
    "        \"ALL\": X.columns.tolist(),\n",
    "        \"MAIN\": get_main_feature_subset_columns(X)\n",
    "    }\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    # -------------------------\n",
    "    # 9-9) آموزش برای هر Feature Set\n",
    "    # -------------------------\n",
    "    for fs_name, fs_cols in feature_sets.items():\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"Feature Set: {fs_name} | #cols={len(fs_cols)}\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        Xtr = X_train[fs_cols].copy()\n",
    "        Xte = X_test[fs_cols].copy()\n",
    "\n",
    "        # ساخت preprocessor برای همین feature set\n",
    "        preprocessor = build_preprocessor(Xtr)\n",
    "\n",
    "        # -------------------------\n",
    "        # Random Forest (leaf 100/300)\n",
    "        # -------------------------\n",
    "        rf_results, rf_models = train_random_forest_models(\n",
    "            Xtr, y_train, Xte, y_test,\n",
    "            feature_set_name=fs_name,\n",
    "            preprocessor=preprocessor\n",
    "        )\n",
    "        all_results.extend(rf_results)\n",
    "\n",
    "        # یک مدل RF را برای گزارش importance انتخاب می‌کنیم (مثلاً leaf=300)\n",
    "        rf_key = f\"RF_leaf300_{fs_name}\"\n",
    "        if rf_key in rf_models:\n",
    "            # گزارش permutation importance (اختیاری و برای تحلیل)\n",
    "            top_ohe_features = select_top_features_with_permutation_importance(\n",
    "                rf_models[rf_key],\n",
    "                Xte,\n",
    "                y_test,\n",
    "                top_k=30\n",
    "            )\n",
    "            # ذخیره top features\n",
    "            with open(os.path.join(ARTIFACT_DIR, f\"top_features_ohe_{fs_name}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(top_ohe_features, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "            # بررسی Bias ساده بر اساس gender/race/age اگر وجود داشته باشد\n",
    "            for col in [\"gender\", \"race\", \"age\"]:\n",
    "                bias_df = subgroup_performance(rf_models[rf_key], Xte, y_test, group_col=col)\n",
    "                if len(bias_df) > 0:\n",
    "                    bias_df.to_csv(os.path.join(ARTIFACT_DIR, f\"bias_subgroup_{rf_key}_{col}.csv\"), index=False)\n",
    "\n",
    "        # -------------------------\n",
    "        # XGBoost (leaves 100/300)\n",
    "        # -------------------------\n",
    "        xgb_results, xgb_models = train_xgboost_models(\n",
    "            Xtr, y_train, Xte, y_test,\n",
    "            feature_set_name=fs_name,\n",
    "            preprocessor=preprocessor\n",
    "        )\n",
    "        all_results.extend(xgb_results)\n",
    "\n",
    "        # Bias check برای XGB هم (اگر leaf300 موجود باشد)\n",
    "        xgb_key = f\"XGB_leaves300_{fs_name}\"\n",
    "        if xgb_key in xgb_models:\n",
    "            for col in [\"gender\", \"race\", \"age\"]:\n",
    "                bias_df = subgroup_performance(xgb_models[xgb_key], Xte, y_test, group_col=col)\n",
    "                if len(bias_df) > 0:\n",
    "                    bias_df.to_csv(os.path.join(ARTIFACT_DIR, f\"bias_subgroup_{xgb_key}_{col}.csv\"), index=False)\n",
    "\n",
    "    # -------------------------\n",
    "    # 9-10) ذخیره نتایج کلی\n",
    "    # -------------------------\n",
    "    with open(os.path.join(ARTIFACT_DIR, \"all_results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # همچنین یک جدول خلاصه هم ذخیره می‌کنیم\n",
    "    summary_rows = []\n",
    "    for r in all_results:\n",
    "        summary_rows.append({\n",
    "            \"model\": r[\"model_name\"],\n",
    "            \"roc_auc\": r.get(\"roc_auc\"),\n",
    "            \"pr_auc\": r.get(\"pr_auc\"),\n",
    "            \"sensitivity_recall_class1\": r.get(\"sensitivity_recall_class_1\")\n",
    "        })\n",
    "    summary_df = pd.DataFrame(summary_rows).sort_values(\n",
    "        by=[\"pr_auc\", \"sensitivity_recall_class1\"],\n",
    "        ascending=False\n",
    "    )\n",
    "    summary_df.to_csv(os.path.join(ARTIFACT_DIR, \"summary_metrics.csv\"), index=False)\n",
    "\n",
    "    print(\"\\n=== DONE ===\")\n",
    "    print(\"Artifacts saved to:\", ARTIFACT_DIR)\n",
    "    print(\"\\nTop models by PR-AUC / Sensitivity:\")\n",
    "    print(summary_df.head(10).to_string(index=False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
